{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "import creds # Credentials from Twitter are saved in this file as creds.py\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token to access Twitter API\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(creds.Bearer_Token)}\n",
    "    return headers\n",
    "\n",
    "# The query to download the data from Twitter\n",
    "def create_url(keyword, start_date, end_date, max_results = 15):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #Downloading query parameters\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "# Function to connect to the API\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To download daily data we need a loop, this function creates the lists used in the loop\n",
    "\n",
    "#Format: Month/Days/Years\n",
    "'''Start: first day of month\n",
    "    End: First day of next month '''\n",
    "\n",
    "daterange = pd.date_range(start='01/01/2019', end='03/01/2019').strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "start_list = []\n",
    "end_list = []\n",
    "\n",
    "for i in range(0,len(daterange)-1):\n",
    "    start_list.append(daterange[i])\n",
    "    end_list.append(daterange[i+1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = creds.Bearer_Token    #Bearer token is saved in creds.py\n",
    "headers = create_headers(bearer_token) \n",
    "keyword = \"cardano -is:retweet -giveaway -ethereum -bitcoin lang:en\" # query tp download cardano tweets\n",
    "\n",
    "max_results = 500\n",
    "\n",
    "total_tweets = 0\n",
    "\n",
    "# Loop to download data\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0                       # Counting tweets per time period\n",
    "    max_count = 1000                # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "        # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "            # Next token is used to reach more tweets\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                filename = (f'{start_list[i]}'.replace(\":\", \"-\") + f'{next_token}' + '.json')\n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(json_response, f)                      #Create JSON for the downloaded data\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(3)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                filename = (f'{start_list[i]}'.replace(\":\", \"-\") + f'{next_token}' + '.json')\n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(json_response, f)                      #Create JSON for the downloaded data\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(3)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(3)\n",
    "\n",
    "print(\"Total number of results: \", total_tweets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "- https://github.com/JustAnotherArchivist/snscrape/issues/459\n",
    "\n",
    "- https://mihaelagrigore.medium.com/scraping-historical-tweets-with-twitter-api-v2-3f55e7263d33#4.-Scraping-rate-limits\n",
    "\n",
    "- https://developer.twitter.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737bd3e0976d60ca799851c839f91960efb19d361293b4772c927e7c090f3b9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
